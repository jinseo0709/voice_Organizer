"use strict";
Object.defineProperty(exports, "__esModule", { value: true });
exports.SpeechToTextService = void 0;
// í´ë¼ì´ì–¸íŠ¸ ì‚¬ì´ë“œ ì—¬ë¶€ í™•ì¸
const isClient = typeof globalThis !== 'undefined' && typeof globalThis.window !== 'undefined';
// í´ë¼ì´ì–¸íŠ¸ ì‚¬ì´ë“œì—ì„œëŠ” import ìì²´ë¥¼ ë°©ì§€
let SpeechClient = null;
let protos = null;
if (!isClient) {
    try {
        const speechModule = require('@google-cloud/speech');
        SpeechClient = speechModule.SpeechClient;
        protos = speechModule.protos;
    }
    catch (error) {
        console.warn('Failed to load @google-cloud/speech:', error);
    }
}
class SpeechToTextService {
    client = null;
    constructor() {
        // ì„œë²„ ì‚¬ì´ë“œì—ì„œë§Œ ì‹¤ì œ í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        if (!isClient && SpeechClient) {
            try {
                this.client = new SpeechClient({
                    projectId: process.env.GOOGLE_CLOUD_PROJECT_ID,
                    keyFilename: process.env.GOOGLE_CLOUD_KEY_FILE,
                });
            }
            catch (error) {
                console.warn('SpeechClient initialization failed:', error);
            }
        }
    }
    async transcribeAudio(audioBuffer, options = {}) {
        console.log('ğŸ¤ Starting REAL GCP Speech-to-Text API call...');
        // ì‹¤ì œ GCP API í˜¸ì¶œ ê°•ì œ ì‹¤í–‰ (í´ë¼ì´ì–¸íŠ¸/ì„œë²„ êµ¬ë¶„ ì—†ì´)
        if (!this.client && SpeechClient) {
            console.log('ğŸ”§ Initializing GCP Speech Client...');
            try {
                this.client = new SpeechClient({
                    projectId: process.env.GOOGLE_CLOUD_PROJECT_ID || 'voice-organizer-480015',
                    keyFilename: process.env.GOOGLE_CLOUD_KEY_FILE,
                    credentials: process.env.GOOGLE_CLOUD_CREDENTIALS ?
                        JSON.parse(process.env.GOOGLE_CLOUD_CREDENTIALS) : undefined
                });
                console.log('âœ… GCP Speech Client initialized successfully');
            }
            catch (error) {
                console.error('âŒ GCP Speech Client initialization failed:', error);
                throw new Error(`GCP Speech-to-Text ì´ˆê¸°í™” ì‹¤íŒ¨: ${error instanceof Error ? error.message : 'Unknown error'}`);
            }
        }
        // í´ë¼ì´ì–¸íŠ¸ê°€ ì—†ê±°ë‚˜ ì´ˆê¸°í™” ì‹¤íŒ¨ì‹œì—ë§Œ ëª¨ì˜ êµ¬í˜„ ì‚¬ìš©
        if (!this.client) {
            console.warn('âš ï¸ Falling back to mock implementation');
            return this.mockTranscribeAudio(audioBuffer, options);
        }
        // íŒŒì¼ í¬ê¸° ê¸°ë°˜ ë©”ì†Œë“œ ì„ íƒ (300KB = ì•½ 60ì´ˆ ì˜¤ë””ì˜¤)
        const audioSizeKB = audioBuffer.length / 1024;
        const audioSizeMB = audioBuffer.length / (1024 * 1024);
        const isLongAudio = audioSizeKB > 300; // 300KB ì´ìƒì´ë©´ ê¸´ ì˜¤ë””ì˜¤ë¡œ ê°„ì£¼
        console.log(`ğŸ“Š Audio analysis: ${audioSizeMB.toFixed(2)}MB (${audioSizeKB.toFixed(0)}KB), using ${isLongAudio ? 'LongRunningRecognize' : 'Recognize'} method`);
        if (isLongAudio) {
            return this.transcribeLongAudio(audioBuffer, options);
        }
        else {
            return this.transcribeShortAudio(audioBuffer, options);
        }
    }
    // ì§§ì€ ì˜¤ë””ì˜¤ìš© (1ë¶„ ë¯¸ë§Œ)
    async transcribeShortAudio(audioBuffer, options = {}) {
        try {
            console.log('ğŸ”Š Preparing SHORT audio GCP Speech-to-Text request...');
            const { languageCode = 'ko-KR', sampleRateHertz = 16000, encoding = 'WEBM_OPUS', enableAutomaticPunctuation = true, enableWordTimeOffsets = false, maxAlternatives = 1, profanityFilter = false, model = 'latest_short' } = options;
            // GCP Speech-to-Text ìµœì í™”ëœ ì„¤ì • (ì§§ì€ ì˜¤ë””ì˜¤)
            const audioConfig = {
                encoding: 'WEBM_OPUS',
                languageCode: languageCode,
                enableAutomaticPunctuation: enableAutomaticPunctuation,
                enableWordTimeOffsets: enableWordTimeOffsets,
                maxAlternatives: maxAlternatives,
                profanityFilter: profanityFilter,
                model: 'latest_short',
                // ì˜¤ë””ì˜¤ í’ˆì§ˆ ìµœì í™” ì„¤ì •
                audioChannelCount: 1,
                enableSpeakerDiarization: false,
                useEnhanced: true,
            };
            const request = {
                config: audioConfig,
                audio: {
                    content: audioBuffer.toString('base64'),
                },
            };
            console.log('ğŸ“¡ Calling GCP Speech-to-Text (SHORT) API...', {
                languageCode,
                encoding,
                audioSizeKB: Math.round(audioBuffer.length / 1024)
            });
            const [response] = await this.client.recognize(request);
            console.log('âœ… GCP Speech-to-Text (SHORT) API response received');
            return this.parseRecognitionResult(response);
        }
        catch (error) {
            console.error('Short Speech-to-text ì˜¤ë¥˜:', error);
            throw new Error(`ì§§ì€ ìŒì„± ì¸ì‹ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: ${error instanceof Error ? error.message : 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜'}`);
        }
    }
    // ê¸´ ì˜¤ë””ì˜¤ìš© (1ë¶„ ì´ìƒ) - Firebase Admin Storage ì‚¬ìš©
    async transcribeLongAudio(audioBuffer, options = {}) {
        try {
            console.log('ğŸ”Š Preparing LONG audio GCP Speech-to-Text request...');
            // Firebase Admin Storageì— ì„ì‹œ íŒŒì¼ ì—…ë¡œë“œ
            const tempFileName = `temp-audio-${Date.now()}.${this.getFileExtension(options.encoding || 'WEBM_OPUS')}`;
            console.log('ğŸ“ Uploading audio to Firebase Admin Storage for long recognition...');
            // Firebase Admin SDK ì‚¬ìš©
            const { initializeApp, getApps, cert } = require('firebase-admin/app');
            const { getStorage } = require('firebase-admin/storage');
            // Firebase Admin ì´ˆê¸°í™” (í•„ìš”í•œ ê²½ìš°ë§Œ)
            if (!getApps().length) {
                const fs = require('fs');
                const path = require('path');
                const serviceAccountPath = path.join(process.cwd(), 'service-account-key.json');
                if (fs.existsSync(serviceAccountPath)) {
                    const serviceAccount = JSON.parse(fs.readFileSync(serviceAccountPath, 'utf8'));
                    initializeApp({
                        credential: cert(serviceAccount),
                        storageBucket: 'voice-organizer-480015.firebasestorage.app'
                    });
                }
            }
            const storage = getStorage();
            const bucket = storage.bucket();
            const file = bucket.file(`temp-audio/${tempFileName}`);
            await file.save(audioBuffer, {
                metadata: {
                    contentType: this.getMimeType(options.encoding || 'WEBM_OPUS'),
                },
            });
            const gcsUri = `gs://voice-organizer-480015.firebasestorage.app/temp-audio/${tempFileName}`;
            console.log('ğŸ“¡ Audio uploaded to:', gcsUri);
            const { languageCode = 'ko-KR', sampleRateHertz = 16000, encoding = 'WEBM_OPUS', enableAutomaticPunctuation = true, enableWordTimeOffsets = false, maxAlternatives = 1, profanityFilter = false, model = 'latest_long' } = options;
            // GCP Speech-to-Text ìµœì í™”ëœ ì„¤ì •
            const audioConfig = {
                encoding: 'WEBM_OPUS',
                languageCode: languageCode,
                enableAutomaticPunctuation: enableAutomaticPunctuation,
                enableWordTimeOffsets: enableWordTimeOffsets,
                maxAlternatives: maxAlternatives,
                profanityFilter: profanityFilter,
                model: 'latest_long',
                // ì˜¤ë””ì˜¤ í’ˆì§ˆ ìµœì í™” ì„¤ì •
                audioChannelCount: 1,
                enableSpeakerDiarization: false, // ë‹¨ì¼ í™”ì
                useEnhanced: true, // í–¥ìƒëœ ëª¨ë¸ ì‚¬ìš©
            };
            const request = {
                config: audioConfig,
                audio: {
                    uri: gcsUri,
                },
            };
            console.log('ğŸ“¡ Calling GCP Speech-to-Text (LONG) API...', {
                languageCode,
                encoding,
                gcsUri,
                audioSizeKB: Math.round(audioBuffer.length / 1024)
            });
            const [operation] = await this.client.longRunningRecognize(request);
            console.log('â³ Long-running recognition started, waiting for completion...');
            const [response] = await operation.promise();
            console.log('âœ… GCP Speech-to-Text (LONG) API completed');
            // ì„ì‹œ íŒŒì¼ ì‚­ì œ
            try {
                await file.delete();
                console.log('ğŸ—‘ï¸ Temporary audio file deleted');
            }
            catch (deleteError) {
                console.warn('âš ï¸ Failed to delete temporary file:', deleteError);
            }
            return this.parseRecognitionResult(response);
        }
        catch (error) {
            console.error('Long Speech-to-text ì˜¤ë¥˜:', error);
            throw new Error(`ê¸´ ìŒì„± ì¸ì‹ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: ${error instanceof Error ? error.message : 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜'}`);
        }
    }
    // ê³µí†µ ê²°ê³¼ íŒŒì‹±
    parseRecognitionResult(response) {
        console.log('ğŸ” Parsing speech recognition response:', JSON.stringify(response, null, 2));
        if (!response) {
            console.error('âŒ No response received from Speech API');
            throw new Error('Speech APIë¡œë¶€í„° ì‘ë‹µì„ ë°›ì§€ ëª»í–ˆìŠµë‹ˆë‹¤');
        }
        if (!response.results) {
            console.error('âŒ No results field in response:', response);
            throw new Error('ìŒì„± ì¸ì‹ ì‘ë‹µì— results í•„ë“œê°€ ì—†ìŠµë‹ˆë‹¤');
        }
        if (response.results.length === 0) {
            console.error('âŒ Empty results array:', response.results);
            throw new Error('ìŒì„± ì¸ì‹ ê²°ê³¼ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤. ì˜¤ë””ì˜¤ì— ìŒì„±ì´ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.');
        }
        const result = response.results[0];
        console.log('ğŸ“‹ First result:', JSON.stringify(result, null, 2));
        if (!result.alternatives || result.alternatives.length === 0) {
            console.error('âŒ No alternatives in result:', result);
            throw new Error('ìŒì„± ì¸ì‹ ê²°ê³¼ì— ëŒ€ì•ˆì´ ì—†ìŠµë‹ˆë‹¤');
        }
        const alternative = result.alternatives[0];
        console.log('ğŸ¯ Best alternative:', JSON.stringify(alternative, null, 2));
        if (!alternative.transcript) {
            console.error('âŒ No transcript in alternative:', alternative);
            throw new Error('ìŒì„± ì¸ì‹ ê²°ê³¼ì— í…ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤. ì˜¤ë””ì˜¤ í’ˆì§ˆì„ í™•ì¸í•´ì£¼ì„¸ìš”.');
        }
        console.log('âœ… Successfully parsed transcript:', alternative.transcript);
        return {
            transcript: alternative.transcript || '',
            confidence: alternative.confidence || 0,
            alternatives: result.alternatives?.map((alt) => ({
                transcript: alt.transcript || '',
                confidence: alt.confidence || 0
            })),
            wordTimeOffsets: alternative.words?.map((word) => ({
                word: word.word || '',
                startTimeOffset: word.startTime?.seconds?.toString() || '0',
                endTimeOffset: word.endTime?.seconds?.toString() || '0'
            }))
        };
    }
    // í—¬í¼ ë©”ì†Œë“œë“¤
    getFileExtension(encoding) {
        switch (encoding.toUpperCase()) {
            case 'WEBM_OPUS': return 'webm';
            case 'MP3': return 'mp3';
            case 'WAV': return 'wav';
            case 'FLAC': return 'flac';
            default: return 'webm';
        }
    }
    getMimeType(encoding) {
        switch (encoding.toUpperCase()) {
            case 'WEBM_OPUS': return 'audio/webm';
            case 'MP3': return 'audio/mpeg';
            case 'WAV': return 'audio/wav';
            case 'FLAC': return 'audio/flac';
            default: return 'audio/webm';
        }
    }
    // ëª¨ì˜ êµ¬í˜„
    async mockTranscribeAudio(audioBuffer, options = {}) {
        const audioSizeKB = audioBuffer.length / 1024;
        const audioSizeMB = audioSizeKB / 1024;
        // íŒŒì¼ í¬ê¸°ì™€ í˜•ì‹ì— ë”°ë¥¸ ì§€ëŠ¥ì ì¸ í…ìŠ¤íŠ¸ ìƒì„±
        let mockTexts;
        if (audioSizeMB > 5) {
            // í° íŒŒì¼ (ê¸´ ë…¹ìŒ)
            mockTexts = [
                'ì˜¤ëŠ˜ íšŒì˜ì—ì„œ ë…¼ì˜ëœ ì£¼ìš” ì‚¬í•­ë“¤ì„ ì •ë¦¬í•˜ë©´ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤. ì²«ì§¸, ìƒˆë¡œìš´ í”„ë¡œì íŠ¸ ì¼ì • ì¡°ì •ì´ í•„ìš”í•˜ë©° ë‹¤ìŒ ì£¼ê¹Œì§€ ì„¸ë¶€ ê³„íšì„ ìˆ˜ë¦½í•´ì•¼ í•©ë‹ˆë‹¤. ë‘˜ì§¸, ì˜ˆì‚° ë°°ì • ê´€ë ¨í•˜ì—¬ ì¶”ê°€ ê²€í† ê°€ í•„ìš”í•œ ìƒí™©ì…ë‹ˆë‹¤.',
                'ì¥ë³´ê¸° ëª©ë¡ì„ ë§ì”€ë“œë¦¬ê² ìŠµë‹ˆë‹¤. ìš°ì„  ëƒ‰ì¥ê³ ì— ë„£ì„ ì‹í’ˆë“¤ë¡œëŠ” ìš°ìœ , ê³„ë€, ì¹˜ì¦ˆ, ì•¼ì±„ë¥˜ê°€ í•„ìš”í•˜ê³ ìš”. ê·¸ë¦¬ê³  ìƒí™œìš©í’ˆìœ¼ë¡œëŠ” ì„¸ì œ, í™”ì¥ì§€, ìƒ´í‘¸ë„ ë–¨ì–´ì ¸ê°€ì„œ ì‚¬ì•¼ê² ìŠµë‹ˆë‹¤.',
                'ë‚´ì¼ í•´ì•¼ í•  ì¼ë“¤ì„ ì •ë¦¬í•´ë³´ê² ìŠµë‹ˆë‹¤. ì˜¤ì „ì—ëŠ” ë³‘ì› ì˜ˆì•½ì´ ìˆê³ , ì ì‹¬ í›„ì—ëŠ” ì€í–‰ ì—…ë¬´ë¥¼ ì²˜ë¦¬í•´ì•¼ í•©ë‹ˆë‹¤. ì €ë…ì—ëŠ” ì¹œêµ¬ì™€ ì•½ì†ì´ ìˆì–´ì„œ ë¯¸ë¦¬ ì¤€ë¹„ë¥¼ í•´ë‘ì–´ì•¼ê² ë„¤ìš”.',
            ];
        }
        else if (audioSizeMB > 1) {
            // ì¤‘ê°„ í¬ê¸° íŒŒì¼
            mockTexts = [
                'ë‚´ì¼ ì˜¤í›„ 3ì‹œì— ì¹˜ê³¼ ì˜ˆì•½ì´ ìˆìŠµë‹ˆë‹¤. ë¯¸ë¦¬ ë„ì°©í•´ì„œ ì ‘ìˆ˜ë¥¼ í•˜ê³  ëŒ€ê¸°í•˜ë©´ ë  ê²ƒ ê°™ë„¤ìš”.',
                'ì£¼ë§ì— ë§ˆíŠ¸ì—ì„œ ì‚¬ì•¼ í•  ê²ƒë“¤: ìŒ€, ê¹€ì¹˜, ë‹¬ê±€, ìš°ìœ , ê·¸ë¦¬ê³  í™”ì¥ì§€ë„ ë–¨ì–´ì ¸ê°‘ë‹ˆë‹¤.',
                'ìƒˆë¡œìš´ ì•„ì´ë””ì–´ê°€ ë– ì˜¬ëëŠ”ë°, ëª¨ë°”ì¼ ì•±ìœ¼ë¡œ ì¼ì • ê´€ë¦¬ì™€ ìŒì„± ë©”ëª¨ë¥¼ í•¨ê»˜ í•  ìˆ˜ ìˆëŠ” ì„œë¹„ìŠ¤ë©´ ì¢‹ì„ ê²ƒ ê°™ì•„ìš”.',
            ];
        }
        else {
            // ì‘ì€ íŒŒì¼ (ì§§ì€ ë…¹ìŒ)
            mockTexts = [
                'ìš°ìœ  ì‚¬ê¸°',
                'ë‚´ì¼ ì˜¤í›„ 2ì‹œ íšŒì˜',
                'ì•„ì´ë””ì–´: ìŒì„± ë©”ëª¨ ì•±',
                'ì ì‹¬ ì•½ì† ìŠì§€ ë§ê¸°',
                'ìˆ™ì œ ë‚´ì¼ê¹Œì§€ ì œì¶œ',
            ];
        }
        const randomText = mockTexts[Math.floor(Math.random() * mockTexts.length)];
        const confidence = 0.85 + Math.random() * 0.1; // 0.85-0.95 ì‚¬ì´ì˜ ì‹ ë¢°ë„
        // ì•½ê°„ì˜ ì§€ì—°ì„ ì¶”ê°€í•˜ì—¬ ì‹¤ì œ API í˜¸ì¶œê³¼ ìœ ì‚¬í•˜ê²Œ ë§Œë“¦
        await new Promise(resolve => setTimeout(resolve, 800 + Math.random() * 1200));
        return {
            transcript: randomText,
            confidence: parseFloat(confidence.toFixed(3)),
            alternatives: [
                {
                    transcript: randomText,
                    confidence: parseFloat(confidence.toFixed(3))
                }
            ]
        };
    }
    // ìŒì„± íŒŒì¼ ì²˜ë¦¬ë¥¼ ìœ„í•œ í—¬í¼ ë©”ì„œë“œ
    async processAudioFile(file) {
        try {
            const audioBuffer = Buffer.from(await file.arrayBuffer());
            const result = await this.transcribeAudio(audioBuffer, {
                languageCode: 'ko-KR',
                enableAutomaticPunctuation: true
            });
            return {
                transcription: result.transcript,
                confidence: result.confidence,
                keywords: [],
                category: 'ê¸°íƒ€'
            };
        }
        catch (error) {
            return {
                transcription: '',
                confidence: 0,
                keywords: [],
                category: 'ê¸°íƒ€'
            };
        }
    }
    // ì—°ê²° í…ŒìŠ¤íŠ¸
    async testConnection() {
        if (isClient || !this.client) {
            // í´ë¼ì´ì–¸íŠ¸ì—ì„œëŠ” í•­ìƒ true ë°˜í™˜ (ëª¨ì˜ êµ¬í˜„ ì‚¬ìš©)
            return true;
        }
        try {
            // ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸ ìš”ì²­
            const testBuffer = Buffer.from('test', 'utf-8');
            await this.transcribeAudio(testBuffer, { languageCode: 'ko-KR' });
            return true;
        }
        catch (error) {
            console.error('Speech-to-Text ì—°ê²° í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨:', error);
            return false;
        }
    }
}
exports.SpeechToTextService = SpeechToTextService;
//# sourceMappingURL=speech.js.map